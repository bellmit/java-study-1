# 数据结构与算法入门

# 课程介绍

## 课程目标
数据结构和算法这门课程无论在哪个学校的计算机专业，都是一门必修课，因为这门课程非常重要的，是编程必备的基础，但是这门课程是一门不太好学习的课程，因为它学习起来是非常的枯燥乏味的。但是如果你想让自己的编程能力有质的飞跃，不再停留于调用现成的API，而是追求更完美的实现，那么这门课程就是你的必修课，因为程序设计=数据结构+算法。
通过对基础数据结构和算法的学习，能更深层次的理解程序，提升编写代码的能力，让程序的代码更优雅，性能更高。

## 课程内容

1. 数据结构和算法概述
2. 算法分析
3. 排序
4. 线性表
5. 符号表
6. 树
7. 堆
8. 优先队列
9. 并查集
10. 图

# 数据结构和算法概述

## 什么是数据结构？

### 官方解释
数据结构是一门研究非数值计算的程序设计问题中的操作对象，以及他们之间的关系和操作等相关问题的学科。

### 大白话

数据结构就是把数据元素按照一定的关系组织起来的集合，用来组织和存储数据

## 数据结构分类

传统上，我们可以把数据结构分为逻辑结构和物理结构两大类。

### 逻辑结构分类

逻辑结构是从具体问题中抽象出来的模型，是抽象意义上的结构，按照对象中数据元素之间的相互关系分类，也是我们后面课题中需要关注和讨论的问题。

#### 集合结构

> 集合结构中数据元素除了属于同一个集合外，他们之间没有任何其他的关系。

![image-20211124180055852](https://cdn.jsdelivr.net/gh/gcdd1993/image-repo/img/20211124180058.png)

#### 线性结构

> 线性结构中的数据元素之间存在一对一的关系

![image-20211124180122360](https://cdn.jsdelivr.net/gh/gcdd1993/image-repo/img/20211124180124.png)

#### 树形结构

> 树形结构中的数据元素之间存在一对多的层次关系

![image-20211124180146911](https://cdn.jsdelivr.net/gh/gcdd1993/image-repo/img/20211124180148.png)

#### 图形结构

> 图形结构的数据元素是多对多的关系

![image-20211124180208982](https://cdn.jsdelivr.net/gh/gcdd1993/image-repo/img/20211124180211.png)

### 物理结构分类

逻辑结构在计算机中真正的表示方式（又称为映像）称为物理结构，也可以叫做存储结构。

常见的物理结构有顺序存储结构、链式存储结构。

#### 顺序存储结构

> 把数据元素放到地址连续的存储单元里面，其数据间的逻辑关系和物理关系是一致的 ，比如我们常用的数组就是顺序存储结构。

![image-20211124180257053](https://cdn.jsdelivr.net/gh/gcdd1993/image-repo/img/20211124180259.png)

顺序存储结构存在一定的弊端，就像生活中排时也会有人插队也可能有人有特殊情况突然离开，这时候整个结构都处于变化中，此时就需要链式存储结构。

#### 链式存储结构

> 是把数据元素存放在任意的存储单元里面，这组存储单元可以是连续的也可以是不连续的。此时，数据元素之间并不能反映元素间的逻辑关系，因此在链式存储结构中引进了一个**指针**存放数据元素的地址，这样通过地址就可以找到相关联数据元素的位置。

![image-20211124180349804](https://cdn.jsdelivr.net/gh/gcdd1993/image-repo/img/20211124180351.png)

## 什么是算法？

### 官方解释

算法是指解题方案的准确而完整的描述，是一系列解决问题的清晰指令，算法代表着用系统的方法解决问题的策略机制。也就是说，能够对一定规范的输入，在有限时间内获得所要求的输出。

### 大白话

根据一定的条件，对一些数据进行计算，得到需要的结果。

## 算法初体验

在生活中，我们如果遇到某个问题，常常解决方案不是唯一的。
例如从西安到北京，如何去？会有不同的解决方案，我们可以坐飞机，可以坐火车，可以坐汽车，甚至可以步行，不同的解决方案带来的时间成本和金钱成本是不一样的，比如坐飞机用的时间最少，但是费用最高，步行费用最低，但时间最长。
再例如在北京二环内买一套四合院，如何付款？也会有不同的解决方案，可以一次性现金付清，也可以通过银行做按揭。这两种解决方案带来的成本也不一样，一次性付清，虽然当时出的钱多，压力大，但是没有利息，按揭虽然当时出的钱少，压力比较小，但是会有利息，而且30年的总利息几乎是贷款额度的一倍，需要多付钱。
在程序中，我们也可以用不同的算法解决相同的问题，而不同的算法的成本也是不相同的。总体上，一个优秀的算法追求以下两个目标：

1. 花最少的时间完成需求；
2. 占用最少的内存空间完成需求；

下面我们用一些实际案例体验一些算法。

### 需求1

计算1到100的和。

第一种解法：

```java
public static void main(String[] args) {
    int sum = 0;
    int n=100;
    for (int i = 1; i <= n; i++) {
    	sum += i;
    }
    System.out.println("sum=" + sum);
}
```

第二种解法：

```java
public static void main(String[] args) {
    int sum = 0;
    int n=100;
    sum = (n+1)*n/2;
    System.out.println("sum="+sum);
}
```

第一种解法要完成需求，要完成以下几个动作：

1. 定义两个整型变量；
2. 执行100次加法运算；
3. 打印结果到控制台；

第二种解法要完成需求，要完成以下几个动作：

1. 定义两个整型变量；
2. 执行1次加法运算，1次乘法运算，一次除法运算，总共3次运算；
3. 打印结果到控制台；

很明显，第二种算法完成需求，花费的时间更少一些。

### 需求2

计算10的阶乘

第一种解法：

```java
public class Test {
    public static void main(String[] args) {
        //测试，计算10的阶乘
        long result = fun1(10);
        System.out.println(result);
    }
    //计算n的阶乘
    public static long fun1(long n){
        if (n==1){
        	return 1;
        }
        return n*fun1(n-1);
    }
}
```

第二种解法：

```java
public class Test {
    public static void main(String[] args) {
        //测试，计算10的阶乘
        long result = fun2(10);
        System.out.println(result);
    }
    //计算n的阶乘
    public static long fun2(long n){
        int result=1;
        for (long i = 1; i <= n; i++) {
        	result*=i;
        }
        return result;
    }
}
```

第一种解法，使用递归完成需求，`fun1`方法会执行10次，并且第一次执行未完毕，调用第二次执行，第二次执行未完毕，调用第三次执行...最终，最多的时候，需要在栈内存同时开辟10块内存分别执行10个`fun1`方法。
第二种解法，使用`for`循环完成需求，`fun2`方法只会执行一次，最终，只需要在栈内存开辟一块内存执行`fun2`方法即可。
很明显，第二种算法完成需求，占用的内存空间更小。

# 算法分析

前面我们已经介绍了，研究算法的最终目的就是如何花更少的时间，如何占用更少的内存去完成相同的需求，并且也通过案例演示了不同算法之间时间耗费和空间耗费上的差异，但我们并不能将时间占用和空间占用量化，因此，接下来我们要学习有关算法时间耗费和算法空间耗费的描述和分析。有关算法时间耗费分析，我们称之为算法的时间复杂度分析，有关算法的空间耗费分析，我们称之为算法的空间复杂度分析。

## 算法的时间复杂度分析

我们要计算算法时间耗费情况，首先我们得度量算法的执行时间，那么如何度量呢？

### 事后分析估算方法

比较容易想到的方法就是我们把算法执行若干次，然后拿个计时器在旁边计时，这种事后统计的方法看上去的确不错，并且也并非要我们真的拿个计算器在旁边计算，因为计算机都提供了计时的功能。

这种统计方法主要是通过设计好的测试程序和测试数据，利用计算机计时器对不同的算法编制的程序的运行时间进行比较，从而确定算法效率的高低，但是这种方法有很大的缺陷：必须依据算法实现编制好的测试程序，通常要花费大量时间和精力，测试完了如果发现测试的是非常糟糕的算法，那么之前所做的事情就全部白费了，并且不同的测试环境(硬件环境)的差别导致测试的结果差异也很大。

```java
public static void main(String[] args) {
    long start = System.currentTimeMillis();
    int sum = 0;
    int n = 100;
    for (int i = 1; i <= n; i++) {
        sum += i;
    }
    System.out.println("sum=" + sum);
    long end = System.currentTimeMillis();
    System.out.println(end - start);
}
```

### 事前分析估算方法

在计算机程序编写前，依据统计方法对算法进行估算，经过总结，我们发现一个高级语言编写的程序程序在计算机上运行所消耗的时间取决于下列因素：

1. 算法采用的策略和方案；
2. 编译产生的代码质量；
3. 问题的输入规模(所谓的问题输入规模就是输入量的多少)；
4. 机器执行指令的速度；

由此可见，抛开这些与计算机硬件、软件有关的因素，一个程序的运行时间依赖于算法的好坏和问题的输入规模。如果算法固定，那么该算法的执行时间就只和问题的输入规模有关系了。

我们再次以之前的求和案例为例，进行分析。

**需求**

计算1到100的和。

第一种解法：

```java
// 如果输入量为n为1，则需要计算1次；
// 如果输入量n为1亿，则需要计算1亿次；
public static void main(String[] args) {
    int sum = 0; // 执行1次
    int n = 100; // 执行1次
    for (int i = 1; i <= n; i++) { // 执行了n+1次
        sum += i; // 执行了n次
    }
    System.out.println("sum=" + sum);
}
```

第二种解法：

```java
// 如果输入量为n为1，则需要计算1次；
// 如果输入量n为1亿，则需要计算1次；
public static void main(String[] args) {
    int sum = 0; // 执行1次
    int n = 100; // 执行1次
    sum = (n + 1) * n / 2; // 执行1次
    System.out.println("sum=" + sum);
}
```

因此，当输入规模为n时，第一种算法执行了`1+1+(n+1)+n=2n+3`次；第二种算法执行了`1+1+1=3`次。如果我们把第一种算法的循环体看做是一个整体，忽略结束条件的判断，那么其实这两个算法运行时间的差距就是n和1的差距。

为什么循环判断在算法1里执行了`n+1`次，看起来是个不小的数量，但是却可以忽略呢？我们来看下一个例子：

**需求**

计算100个1+100个2+100个3+...100个100的结果

代码：

```java
public static void main(String[] args) {
    int sum = 0;
    int n = 100;
    for (int i = 1; i <= n; i++) {
        for (int j = 1; j <= n; j++) {
            sum += i;
        }
    }
    System.out.println("sum=" + sum);
}
```

上面这个例子中，如果我们要精确的研究循环的条件执行了多少次，是一件很麻烦的事情，并且，由于真正计算和的代码是内循环的循环体，所以，在研究算法的效率时，我们只考虑**核心代码的执行次数**，这样可以简化分析。
我们研究算法复杂度，侧重的是当输入规模不断增大时，算法的增长量的一个抽象(规律)，而不是精确地定位需要执行多少次，因为如果是这样的话，我们又得考虑回编译期优化等问题，容易主次跌倒。
我们不关心编写程序所用的语言是什么，也不关心这些程序将跑在什么样的计算机上，我们只关心它所实现的算法。这样，不计那些循环索引的递增和循环终止的条件、变量声明、打印结果等操作，最终在分析程序的运行时间时，最重要的是把程序看做是独立于程序设计语言的算法或一系列步骤。我们分析一个算法的运行时间，最重要的就是把核心操作的次数和输入规模关联起来。

![image-20211130145516291](https://cdn.jsdelivr.net/gh/gcdd1993/image-repo@master/img/202111301455341.png)

### 函数渐近增长

给定两个函数`f(n)`和`g(n)`,如果存在一个整数N，使得对于所有的`n>N`,`f(n)`总是比`g(n)`大，那么我们说`f(n)`的增长渐近快于`g(n)`。
概念似乎有点艰涩难懂，那接下来我们做几个测试。

#### 测试一

假设四个算法的输入规模都是n：

1. 算法A1要做2n+3次操作，可以这么理解：先执行n次循环，执行完毕后，再有一个n次循环，最后有3次运算；
2. 算法A2要做2n次操作；
3. 算法B1要做3n+1次操作，可以这个理解：先执行n次循环，再执行一个n次循环，再执行一个n次循环，最后有1次运算。
4. 算法B2要做3n次操作；

那么，上述算法，哪一个更快一些呢？

![image-20211130145702472](https://cdn.jsdelivr.net/gh/gcdd1993/image-repo@master/img/202111301457554.png)

![image-20211130145721608](https://cdn.jsdelivr.net/gh/gcdd1993/image-repo@master/img/202111301457663.png)

通过数据表格，比较算法A1和算法B1：

- 当输入规模n=1时，A1需要执行5次，B1需要执行4次，所以A1的效率比B1的效率低；
- 当输入规模n=2时，A1需要执行7次，B1需要执行7次，所以A1的效率和B1的效率一样；
- 当输入规模n>2时，A1需要的执行次数一直比B1需要执行的次数少，所以A1的效率比B1的效率高；

所以我们可以得出结论：

**当输入规模n>2时，算法A1的渐近增长小于算法B1 的渐近增长**

通过观察折线图，我们发现，随着输入规模的增大，算法A1和算法A2逐渐重叠到一块，算法B1和算法B2逐渐重叠到一块，所以我们得出结论：

**随着输入规模的增大，算法的常数操作可以忽略不计**

#### 测试二

假设四个算法的输入规模都是n：

1. 算法C1需要做`4n+8`次操作
2. 算法C2需要做n次操作
3. 算法D1需要做`2n^2`次操作
4. 算法D2需要做`n^2`次操作

那么上述算法，哪个更快一些？

![image-20211130150053404](https://cdn.jsdelivr.net/gh/gcdd1993/image-repo@master/img/202111301500450.png)

![image-20211130150114770](https://cdn.jsdelivr.net/gh/gcdd1993/image-repo@master/img/202111301501827.png)

通过数据表格，对比算法C1和算法D1：

- 当输入规模n<=3时，算法C1执行次数多于算法D1，因此算法C1效率低一些；
- 当输入规模n>3时，算法C1执行次数少于算法D1，因此，算法D2效率低一些；

所以，总体上，算法C1要优于算法D1。

通过折线图，对比对比算法C1和C2：

- 随着输入规模的增大，算法C1和算法C2几乎重叠

通过折线图，对比算法C系列和算法D系列：

- 随着输入规模的增大，即使去除n^2前面的常数因子，D系列的次数要远远高于C系列。

因此，可以得出结论：

**随着输入规模的增大，与最高次项相乘的常数可以忽略**

#### 测试三

假设四个算法的输入规模都是n：

- 算法E1：`2n^2+3n+1`
- 算法E2：`n^2`
- 算法F1：`2n^3+3n+1`
- 算法F2：`n^3`

那么上述算法，哪个更快一些？

![image-20211130150438225](https://cdn.jsdelivr.net/gh/gcdd1993/image-repo@master/img/202111301504270.png)

![image-20211130150448095](https://cdn.jsdelivr.net/gh/gcdd1993/image-repo@master/img/202111301504148.png)

通过数据表格，对比算法E1和算法F1：

- 当n=1时，算法E1和算法F1的执行次数一样；
- 当n>1时，算法E1的执行次数远远小于算法F1的执行次数；

所以算法E1总体上是由于算法F1的。

通过折线图我们会看到，算法F系列随着n的增长会变得特块，算法E系列随着n的增长相比较算法F来说，变得比较慢，所以可以得出结论：

**最高次项的指数大的，随着n的增长，结果也会变得增长特别快**

#### 测试四

假设五个算法的输入规模都是n：

- 算法G：`n^3`
- 算法H：`n^2`
- 算法I：`n`
- 算法J：`logn`
- 算法K：`1`

那么上述算法，哪个效率更高呢？

![image-20211130150657738](https://cdn.jsdelivr.net/gh/gcdd1993/image-repo@master/img/202111301506775.png)

![image-20211130150705190](https://cdn.jsdelivr.net/gh/gcdd1993/image-repo@master/img/202111301507249.png)

通过观察数据表格和折线图，很容易可以得出结论：
**算法函数中n最高次幂越小，算法效率越高**

综上所述，在我们比较算法随着输入规模的增长量时，可以有以下规则：

1. 算法函数中的常数可以忽略；
2. 算法函数中最高次幂的常数因子可以忽略；
3. 算法函数中最高次幂越小，算法效率越高。

